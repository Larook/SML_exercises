---
title: "testing of the R functions"
output: pdf_document
---

```{r load libraries and dataset}

library(gmodels)
library(class)
library(caret)
library(swirl)
library(ggplot2)
library(spatstat)


library(rpart)
library(rpart.plot)
library(stats)
library(randomForest)
library(caret)
library(kernlab)
library(RSNNS)
library(neuralnet)


source("important_functions/load_data_id.R")
source("important_functions/get_training_test_data.R")

source("important_functions/get_normalized_data.R")
source("important_functions/get_gaussian_smoothed_data.R")
source("important_functions/view_data.R")

source("important_functions/get_k_clustered_cipher_data.R") 
source("important_functions/get_PCA_reduced_data.R") 

################################
# This version: 
# - plots and computes computational code 
# - changed to 3 hidden layers with 120 nodes like Zhuoqi and (150,100,50) for PCA part

# TODO: add pre-processing from Karol's 
# TODO: Cross-validation 
# TODO: mean and variance
# TODO: a function for computing + plotting run-time maybe instead of copying same code everywhere

# current issue: takes too long to run
################################

# load dataset all_in
id <- load_data_id(load_full=FALSE)

# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)

# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id

# split to train and test dataset
data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=50)
train_data_allin <- data_train_test_allin[[1]]
test_data_allin <- data_train_test_allin[[2]]


# splitting data disjunct
data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
train_data_disjunct <- data_train_test_disjunct[[1]]
test_data_disjunct <- data_train_test_disjunct[[2]]


# decide if doing the all_in or disjunct
# id_train <-train_data_allin
# id_test <- test_data_allin

id_train <-train_data_disjunct
id_test <- test_data_disjunct
```

```{r }
####################################
####### 5.2: Neural Networks ####### 
####################################

#######################################################################################################
## 5.2.1
## Create a matrix (as below in grey background) for the training classes. It has N rows (the same as 
## the number of training data) and 10 columns (binary). The column with '1' marks the 
## corresponding class as the example shown below (eg. cyper '0' is represented by '1000000000').
#######################################################################################################

source("NN_functions/mlp_functions.R") 

trainingClass <- get_training_class(training_data=id_train)

################# test creation of the 1,2,3 hidden layers MLPs #################

network <- train_mlp(id_train, trainingClass, 3)

# network <- train_mlp(id_train, trainingClass, c(2, 2))

# network <- train_mlp(id_train, trainingClass, c(2, 2, 2))


################# evaluate the network
evaluate_nn(network, id_test)

```


```{r }
######################## One hidden layer - check
accuracyTestList <- c()
accuracyTrainingList <- c()
runtimeList <- c()

# neurons <- c(38,40,42)
neurons <- c(2,3,4)

for (i in 1:length(neurons)) {
  start_time <- Sys.time()
  neurons_now <- neurons[i]
  
  network <- train_mlp(id_train, trainingClass, neurons_now)
  
  end_time <- Sys.time()
  runtimeList[i] <- difftime(Sys.time(), start_time, units = "secs")
  
  accuracyTestList[i] <- evaluate_nn(network, id_test)
  accuracyTrainingList[i] <- evaluate_nn(network, id_train)
}
```

```{r }
plot(neurons, unlist(accuracyTrainingList), type="b", col=1, lwd=1, pch=15, xlab="Number of Neurons in Hidden Layer", ylab="Accuracy")
plot_labels <- c("Training Data")

lines(neurons, unlist(accuracyTestList), type="b", col=2, lwd=1, pch=16)
plot_labels[2] <- paste("Test Data")


title("Evaluation of an ANN with One Hidden Layer")
legend("bottomright",plot_labels, lwd=c(1), col=c(1,2), pch=c(15,16), y.intersp=1)

plot(neurons, unlist(runtimeList), type="b", col=1, lwd=1, pch=15, xlab="Number of Neurons in Hidden Layer", ylab="Runtime (s)")
title("NN Training Runtime -- one hidden layer")
```
##TODO: run that 10 times to have cross validation - means and variances of performances etc
https://www.r-bloggers.com/2015/09/fitting-a-neural-network-in-r-neuralnet-package/

    Do the train-test split
    Fit the model to the train set
    Test the model on the test set
    Calculate the prediction error
    Repeat the process K times

```{r cross validation of one model of the network}
# Now the net. Note that I am splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. I am also initializing a progress bar using the plyr
#library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while.



set.seed(450)
cv.error <- NULL
k <- 10
library(plyr) 
pbar <- create_progress_bar('text')
pbar$init(k)


# numbers of neurons of hidden layers to check
nn_sizes_check <- list( c(1), c(1,1), c(1,1,1))


# dataframe saving information about all the trained and evaluated models with CV
df_mlp_full_information <- data.frame()

# specify the used dataset
data <- id_use

for (nn_size in nn_sizes_check){
  
  # get the correct architecture of network
  layer_1 <- nn_size[1]
  layer_2 <- nn_size[2]
  layer_3 <- nn_size[3]
  # print(nn_size)
  layers_now <- c(layer_1, layer_2, layer_3)[!is.na(c(layer_1, layer_2, layer_3))]
  
  if (is.na(layer_1)){
    layer_1 = 0
  }
  if (is.na(layer_2)){
    layer_2 = 0
  }
  if (is.na(layer_3)){
    layer_3 = 0
  }
  # print(layer_1)
  # print(layer_2)
  # print(layer_3)
  
  print("layers_now")
  print(layers_now)

    # do the cross validation
  folds <- createFolds(data[,1], k = 10)
  # save this information
  nn_size_l1 <- c()
  nn_size_l2 <- c()
  nn_size_l3 <- c()
  accuracy_test_list <- c()
  accuracy_training_list <- c()
  time_training_list <- c()
  time_eval_list <- c()

  for(i in 1:10){
    
    # CV splitting from Zouchi knn
    train.cv <- data[-folds[[i]], ]
    test.cv <- data[folds[[i]], ]
    
    # train the MLP and save the time
    start_time <- Sys.time()
    training_class <- get_training_class(training_data = train.cv)
    nn <- train_mlp(training_data=train.cv, training_classes=training_class, size=layers_now)
    time_training_list[i] <- difftime(Sys.time(), start_time, units = "secs")
    
    # evaluate MLP and save the time and accuracy
    start_time <- Sys.time()
    accuracy_test_list[i] <- evaluate_nn(network, test.cv)
    time_eval_list[i] <- difftime(Sys.time(), start_time, units = "secs")
    accuracy_training_list[i] <- evaluate_nn(network, train.cv)
    
    # save the current architecture
    nn_size_l1[i] <- layer_1
    nn_size_l2[i] <- layer_2
    nn_size_l3[i] <- layer_3
  
    pbar$step()
  }
  
  # save results to DF of 1 mlp configuration
  df_mlp_this_architecture <-data.frame(list(
  'nn_size_l1' = nn_size_l1, 
  'nn_size_l2' = nn_size_l2, 
  'nn_size_l3' = nn_size_l3, 
  'accuraccy_test' = accuracy_test_list, 
  'accuracy_training' = accuracy_training_list,
  'time_training' = time_training_list,
  'time_eval' = time_eval_list
  ))
  
  # add current architecture information to full df
  df_mlp_full_information <- rbind(df_mlp_full_information, df_mlp_this_architecture)
  
}

write.csv(df_mlp_full_information,"nn_results/df_mlp_full_information.csv", row.names = FALSE)

```

```{r }


######## playground for testing of writing the df

nn_sizes_check_a <- list( c(1), c(1,1), c(1,1,1))
nn_size_list <- c()


for (nn_size in nn_sizes_check_a){
  
  
  layer_1 <- nn_size[1]
  layer_2 <- nn_size[2]
  layer_3 <- nn_size[3]
  print(nn_size)
 
  
  layers_now <- c(layer_1, layer_2, layer_3)[!is.na(c(layer_1, layer_2, layer_3))]
  
  if (is.na(layer_1)){
    layer_1 = 0
  }
  if (is.na(layer_2)){
    layer_2 = 0
  }
  if (is.na(layer_3)){
    layer_3 = 0
  }
  print(layer_1)
  print(layer_2)
  print(layer_3)
  
  
  
  print("layers_now")
  print(layers_now)
  
  
  nn_size_l1 <- c()
  nn_size_l2 <- c()
  nn_size_l3 <- c()

  
  a <- train_mlp(training_data=train.cv, training_classes=training_class, size=layers_now)
  for(i in 1:10){
    # main CV here
      # a <- train_mlp(training_data=train.cv, training_classes=training_class, size=layers_now)
    
    nn_size_l1[i] <- layer_1
    nn_size_l2[i] <- layer_2
    nn_size_l3[i] <- layer_3
  }
  
  mlp_tmp <-data.frame(list(
  'nn_size_l1' = nn_size_l1, 
  'nn_size_l2' = nn_size_l2, 
  'nn_size_l3' = nn_size_l3 ))

}


```

```{r }
nn_sizes_check_a <- list( c(1), c(1,1), c(1,1,1))

for (nn_size in nn_sizes_check_a){
  layer_1 <- nn_size[1]
  layer_2 <- nn_size[2]
  layer_3 <- nn_size[3]
  print(nn_size)
  print(layer_1)
  print(layer_2)
  print(layer_3)
  
  
  layers_now <- c(layer_1, layer_2, layer_3)[!is.na(c(layer_1, layer_2, layer_3))]
  print("layers_now")
  print(layers_now)
  a <- train_mlp(training_data=train.cv, training_classes=training_class, size=layers_now)
}

```

```{r }
######################## Two hidden layers #########################################################
####################################################################################################
accuracyTestList <- c()
accuracyTrainingList <- c()
runtimeList <- c()

#neurons <- c(2,4,6,8,10,12,14,16,18,20, 22, 24, 26, 28, 30)

neurons <- c(38,40,42,44,46,48,50,52)

for (i in seq(38,52, by = 2)) {  
  start_time <- Sys.time()
  
  size <- (i + 1)
  network <- train_mlp(id_train, trainingClass, c(150,size))
  
  end_time <- Sys.time()
  runtimeList[[i+1]] <- difftime(Sys.time(), start_time, units = "secs")
  
  accuracyTestList[[i+1]] <- evaluate_nn(network, id_test)
  accuracyTrainingList[[i+1]] <- evaluate_nn(network, id_train)
}

plot(neurons, unlist(accuracyTrainingList), type="b", col=2, lwd=0.8, pch=16, xlab="Number of Neurons in Second Hidden Layer", ylab="Accuracy")
plot_labels <- c("Training Data")

# For some reason R doesn't show this second line
lines(neurons, unlist(accuracyTestList), type="b", col=1, lwd=0.8, pch=15)
plot_labels[2] <- paste("Test Data")

title("Evaluation of an ANN with Two Hidden Layers")
legend("bottomright",plot_labels, lwd=c(1), col=c(1,2), pch=c(16,15), y.intersp=1)

plot(neurons, unlist(runtimeList), type="b", col=1, lwd=1, pch=15, xlab="Number of Neurons in Second Hidden Layer", ylab="Runtime (s)")
title("NN Training Runtime -- two hidden layers")

```

```{r }
######################## Three hidden layers #########################################################
####################################################################################################
accuracyTestList <- c()
accuracyTrainingList <- c()
runtimeList <- c()

#neurons <- c(2,4,6,8,10,12,14,16,18,20, 22, 24, 26, 28, 30)

neurons <- c(38,40,42,44,46,48,50,52)

for (i in seq(38,52, by = 2)) {
  
  start_time <- Sys.time()
  
  size <- (i + 1)
  network <- train_mlp(id_train, trainingClass, c(150, 100,size)) #(150,100,50),
  
  end_time <- Sys.time()
  runtimeList[[i+1]] <- difftime(Sys.time(), start_time, units = "secs")
  
  accuracyTestList[[i+1]] <- evaluate_nn(network, id_test)
  accuracyTrainingList[[i+1]] <- evaluate_nn(network, id_train)
}

plot(neurons, unlist(accuracyTrainingList), type="b", col=2, lwd=0.8, pch=16, xlab="Number of Neurons in Third Hidden Layer", ylab="Accuracy")
plot_labels <- c("Training Data")

# For some reason R doesn't show this second line
lines(neurons, unlist(accuracyTestList), type="b", col=1, lwd=0.8, pch=15)
plot_labels[2] <- paste("Test Data")

title("Evaluation of an ANN with Three Hidden Layers")
legend("bottomright",plot_labels, lwd=c(1), col=c(1,2), pch=c(16,15), y.intersp=1)

plot(neurons, unlist(runtimeList), type="b", col=1, lwd=1, pch=15, xlab="Number of Neurons in Second Hidden Layer", ylab="Runtime (s)")
title("NN Training Runtime -- three hidden layers")
```

```{r }
############################################################################################
## 5.2.4
## Training of NN with pre-processed data
############################################################################################

performPCA <- function(dataset, pcaCount){
  pca <- prcomp(dataset,scale=FALSE, rank. = pcaCount) # rank = a number specifying the maximal rank, i.e., maximal number of principal components to be used. 
  return (pca)
}

pca.10 <- performPCA(id[,-1], 10)

accuracyTestList.pca <- c()
accuracyTrainingList.pca <- c()
runtimeList.pca <- c()

neurons <- c(38,40,42,44,46,48,50,52)

for (i in seq(38,52, by = 2)) {
  
  start_time <- Sys.time()
  size <- (i + 1)
  network <- train_mlp(pca.10$x, trainingClass, c(150,100,size)) #c(150,100,50) <- From Zhuoqi
  end_time <- Sys.time()
  
  runtimeList.pca[[i+1]] <- difftime(Sys.time(), start_time, units = "secs")
  
  accuracyTestList.pca[[i+1]] <- evaluate_nn(network, id_test)
  accuracyTrainingList.pca[[i+1]] <- evaluate_nn(network, id_train)
}

plot(neurons, unlist(accuracyTrainingList.pca), type="b", col=1, lwd=1, pch=15, xlab="Number of Neurons in Hidden Layer", ylab="Accuracy")
plot_labels <- c("Training Data")

lines(neurons, unlist(accuracyTestList.pca), type="b", col=2, lwd=1, pch=16)
plot_labels[2] <- paste("Test Data")


title("Evaluation of an ANN w/ PCA Preprocessing and One Hidden Layer")
legend("bottomright",plot_labels, lwd=c(1), col=c(1,2), pch=c(15,16), y.intersp=1)


plot(neurons, unlist(runtimeList), type="b", col=1, lwd=1, pch=15, xlab="Number of Neurons in Second Hidden Layer", ylab="Runtime (s)")
title("NN w/ PCA Preprocessing Training Runtime -- two hidden layers")

```
