library(plotly)
# load dataset all_in
id <- load_data_id(load_full=FALSE)
# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)
# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id
# split to train and test dataset
data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=70)
train_data_allin <- data_train_test_allin[[1]]
test_data_allin <- data_train_test_allin[[2]]
# splitting data disjunct
data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
train_data_disjunct <- data_train_test_disjunct[[1]]
test_data_disjunct <- data_train_test_disjunct[[2]]
# decide if doing the all_in or disjunct
# id_train <-train_data_allin
# id_test <- test_data_allin
# id_train <-train_data_disjunct
# id_test <- test_data_disjunct
train_data <- train_data_allin
test_data <- test_data_allin
# load dataset all_in
id <- load_data_id(load_full=FALSE)
# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)
# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id
# split to train and test dataset
data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=50)
train_data_allin <- data_train_test_allin[[1]]
test_data_allin <- data_train_test_allin[[2]]
# splitting data disjunct
data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
train_data_disjunct <- data_train_test_disjunct[[1]]
test_data_disjunct <- data_train_test_disjunct[[2]]
# decide if doing the all_in or disjunct
# id_train <-train_data_allin
# id_test <- test_data_allin
id_train <-train_data_disjunct
id_test <- test_data_disjunct
train_data<- id_train
test <- id_test
classifier_rbf <-ksvm(V1~ ., data = train, kernel = "polydot", C = 5)
datasetTest <- predict(classifier_rbf,test)
test_confusion <- confusionMatrix(datasetTest, test$V1)
Model_linear <- ksvm(V1~ ., data = train_data, scale = FALSE, kernel = "vanilladot")
Eval_linear<- predict(Model_linear, test)
test_confusion <- confusionMatrix(Eval_linear, test$V1)
paste("confusionMatrix(datasetTest,test$V1) = ", test_confusion$overall[1])
# Accuracy    :  0.972
#______________We use train_datafunction from caret package to perform crossvalidation______________#
trainControl <- trainControl(method="cv", number=5)
metric <- "Accuracy"
set.seed(100)
Model_linear
# ............................... BEST ONE TO USE FOR DIGITS ..............................
# POLYNOMIAL KERNEL
grid <- expand.grid(C=c(0.01, 0.1, 0.2, 0.3, 0.4), degree=2, scale=0.5)
fit.svm <- train(V1~ ., data=train_data, method="svmPoly", metric=metric,
tuneGrid=grid, trControl=trainControl)
# Printing cross validation result
print(fit.svm, zero.print = ".")
# Best tune at C = 0.1,
# Accuracy = 0.9725012
# Plotting "fit.svm" results
plot(fit.svm, ylim = c(0, 1.05))
# plot(fit.svm)
knitr::opts_chunk$set(echo = TRUE)
library("kernlab")
library(readr)
library(caret)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(kernlab)
library(readr)
library(caret)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(plotly)
# # LOAD THE DATA
# load("idList-cornered-100-2021.Rdata")
#
#
# # NORMALIZE FUNCTION
# normalize <- function(x) {
#   return ((x - min(x)) / (max(x) - min(x)))
# }
#
# # DATA SPLITTING INTO TRAIN / TEST PARTS
# id <- do.call(rbind, idList[1:2])
# id <- as.data.frame(id)
# id[,1] <- factor(id[,1])
# id_sn <- as.data.frame(lapply(id[-1], normalize))
# ## 50/50 split of dataset
# smp_size <- floor(0.5 * nrow(id))
# ## set the seed to make your partition reproducible
# set.seed(123)
# train_ind <- sample(seq_len(nrow(id)), size = smp_size)
# train_data<- id[train_ind, ]
# test <- id[-train_ind, ]
source("important_functions/load_data_id.R")
source("important_functions/get_training_test_data.R")
source("important_functions/get_normalized_data.R")
source("important_functions/get_gaussian_smoothed_data.R")
source("important_functions/view_data.R")
source("important_functions/get_k_clustered_cipher_data.R")
source("important_functions/get_PCA_reduced_data.R")
# load dataset all_in
id <- load_data_id(load_full=FALSE)
# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)
# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id
# split to train and test dataset
data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=50)
train_data_allin <- data_train_test_allin[[1]]
test_data_allin <- data_train_test_allin[[2]]
# splitting data disjunct
data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
train_data_disjunct <- data_train_test_disjunct[[1]]
test_data_disjunct <- data_train_test_disjunct[[2]]
# decide if doing the all_in or disjunct
# id_train <-train_data_allin
# id_test <- test_data_allin
id_train <-train_data_disjunct
id_test <- test_data_disjunct
# ----------- map from data obtainin to used variables ----------------
train_data<- id_train
test <- id_test
classifier_rbf <-ksvm(V1~ ., data = train, kernel = "polydot", C = 5)
datasetTest <- predict(classifier_rbf,test)
test_confusion <- confusionMatrix(datasetTest, test$V1)
classifier_rbf <-ksvm(V1~ ., data = train, kernel = "polydot", C = 5)
#classifier_rbf <-ksvm(V1~ ., data = train, kernel = "vanilladot", C = 1)
#classifier_rbf <-ksvm(V1~., data = train, kernel = "rbfdot", kpar=list(sigma=0.05), C = 1)
classifier_rbf <-ksvm(V1~ ., data = train_data, kernel = "polydot", C = 5)
datasetTest <- predict(classifier_rbf,test)
test_confusion <- confusionMatrix(datasetTest, test$V1)
test_confusion
Model_linear <- ksvm(V1~ ., data = train_data, scale = FALSE, kernel = "vanilladot")
Eval_linear<- predict(Model_linear, test)
test_confusion <- confusionMatrix(Eval_linear, test$V1)
paste("confusionMatrix(datasetTest,test$V1) = ", test_confusion$overall[1])
# Accuracy    :  0.972
#______________We use train_datafunction from caret package to perform crossvalidation______________#
trainControl <- trainControl(method="cv", number=5)
metric <- "Accuracy"
set.seed(100)
Model_linear
Model_linear
trainControl
Model_linear
grid <- expand.grid(C=c(0.01, 0.1, 0.2, 0.3, 0.4), degree=2, scale=0.5)
fit.svm <- train(V1~ ., data=train_data, method="svmPoly", metric=metric,
tuneGrid=grid, trControl=trainControl)
# Printing cross validation result
print(fit.svm, zero.print = ".")
# Best tune at C = 0.1,
# Accuracy = 0.9725012
# Plotting "fit.svm" results
plot(fit.svm, ylim = c(0, 1.05))
# plot(fit.svm)
grid <- expand.grid(C=c(0.01, 0.1, 0.2, 0.3, 0.4), degree=2, scale=0.5)
fit.svm <- train(V1~ ., data=train_data, method="svmPoly", metric=metric,
tuneGrid=grid, trControl=trainControl)
# ............................... BEST ONE TO USE FOR DIGITS ..............................
# POLYNOMIAL KERNEL
grid <- expand.grid(C=c(0.01, 0.1, 0.2, 0.3, 0.4), degree=2, scale=0.5)
fit.svm <- train(V1~ ., data=train_data, method="svmPoly", metric=metric,
tuneGrid=grid, trControl=trainControl)
grid <- expand.grid(C=c(0.01, 0.1, 0.2, 0.3, 0.4), degree=2, scale=0.5)
fit.svm <- train(V1~ ., data=train_data, method="svmPoly", metric=metric,
tuneGrid=grid, trControl=trainControl)
# Best tune at C = 0.1,
# Accuracy = 0.9725012
# Plotting "fit.svm" results
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
# Best tune at C = 0.1,
# Accuracy = 0.9725012
# Plotting "fit.svm" results
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
plot(fit.svm, ylim = c(0.97, 1.05))
id_train <-train_data_disjunct
library(gmodels)
library(class)
library(caret)
library(swirl)
library(ggplot2)
library(spatstat)
library(rpart)
library(rpart.plot)
library(stats)
library(randomForest)
library(caret)
library(kernlab)
library(RSNNS)
library(neuralnet)
source("important_functions/load_data_id.R")
source("important_functions/get_training_test_data.R")
source("important_functions/get_normalized_data.R")
source("important_functions/get_gaussian_smoothed_data.R")
source("important_functions/view_data.R")
source("important_functions/get_k_clustered_cipher_data.R")
source("important_functions/get_PCA_reduced_data.R")
################################
# load dataset all_in
id <- load_data_id(load_full=FALSE)
# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)
# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id
# split to train and test dataset
data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=50)
train_data_allin <- data_train_test_allin[[1]]
test_data_allin <- data_train_test_allin[[2]]
# splitting data disjunct
data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
train_data_disjunct <- data_train_test_disjunct[[1]]
test_data_disjunct <- data_train_test_disjunct[[2]]
# decide if doing the all_in or disjunct
# id_train <-train_data_allin
# id_test <- test_data_allin
id_train <-train_data_disjunct
id_test <- test_data_disjunct
####################################
####### 5.2: Neural Networks #######
####################################
#######################################################################################################
## 5.2.1
## Create a matrix (as below in grey background) for the training classes. It has N rows (the same as
## the number of training data) and 10 columns (binary). The column with '1' marks the
## corresponding class as the example shown below (eg. cyper '0' is represented by '1000000000').
#######################################################################################################
source("NN_functions/mlp_functions.R")
trainingClass <- get_training_class(training_data=id_train)
################# test creation of the 1,2,3 hidden layers MLPs #################
network <- train_mlp(id_train, trainingClass, 3)
# network <- train_mlp(id_train, trainingClass, c(2, 2))
# network <- train_mlp(id_train, trainingClass, c(2, 2, 2))
################# evaluate the network
evaluate_nn(network, id_test)
library(gmodels)
library(class)
library(caret)
library(swirl)
library(ggplot2)
library(spatstat)
library(rpart)
library(rpart.plot)
library(stats)
library(randomForest)
library(caret)
library(kernlab)
library(RSNNS)
library(neuralnet)
source("important_functions/load_data_id.R")
source("important_functions/get_training_test_data.R")
source("important_functions/get_normalized_data.R")
source("important_functions/get_gaussian_smoothed_data.R")
source("important_functions/view_data.R")
source("important_functions/get_k_clustered_cipher_data.R")
source("important_functions/get_PCA_reduced_data.R")
################################
# load dataset all_in
# id <- load_data_id(load_full=FALSE)
id <- load_data_id(load_full=TRUE)
# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)
# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id
# split to train and test dataset
data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=50)
train_data_allin <- data_train_test_allin[[1]]
test_data_allin <- data_train_test_allin[[2]]
# splitting data disjunct
data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
train_data_disjunct <- data_train_test_disjunct[[1]]
test_data_disjunct <- data_train_test_disjunct[[2]]
# decide if doing the all_in or disjunct
# id_train <-train_data_allin
# id_test <- test_data_allin
id_train <-train_data_disjunct
id_test <- test_data_disjunct
library(gmodels)
library(class)
library(caret)
library(swirl)
library(ggplot2)
library(spatstat)
library(rpart)
library(rpart.plot)
library(stats)
library(randomForest)
library(caret)
library(kernlab)
library(RSNNS)
library(neuralnet)
source("important_functions/load_data_id.R")
source("important_functions/get_training_test_data.R")
source("important_functions/get_normalized_data.R")
source("important_functions/get_gaussian_smoothed_data.R")
source("important_functions/view_data.R")
source("important_functions/get_k_clustered_cipher_data.R")
source("important_functions/get_PCA_reduced_data.R")
################################
# load dataset all_in
# id <- load_data_id(load_full=FALSE)
id <- load_data_id(load_full=TRUE)
# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)
# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id
# split to train and test dataset
data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=50)
train_data_allin <- data_train_test_allin[[1]]
test_data_allin <- data_train_test_allin[[2]]
# splitting data disjunct
data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
train_data_disjunct <- data_train_test_disjunct[[1]]
test_data_disjunct <- data_train_test_disjunct[[2]]
# decide if doing the all_in or disjunct
id_train <-train_data_allin
id_test <- test_data_allin
# id_train <-train_data_disjunct
# id_test <- test_data_disjunct
####################################
####### 5.2: Neural Networks #######
####################################
#######################################################################################################
## 5.2.1
## Create a matrix (as below in grey background) for the training classes. It has N rows (the same as
## the number of training data) and 10 columns (binary). The column with '1' marks the
## corresponding class as the example shown below (eg. cyper '0' is represented by '1000000000').
#######################################################################################################
source("NN_functions/mlp_functions.R")
trainingClass <- get_training_class(training_data=id_train)
################# test creation of the 1,2,3 hidden layers MLPs #################
network <- train_mlp(id_train, trainingClass, 3)
# network <- train_mlp(id_train, trainingClass, c(2, 2))
# network <- train_mlp(id_train, trainingClass, c(2, 2, 2))
################# evaluate the network
evaluate_nn(network, id_test)
# Now the net. Note that I am splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. I am also initializing a progress bar using the plyr
#library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while.
# numbers of neurons of hidden layers to check
source("NN_functions/mlp_functions.R")
nn_sizes_check <- list(c(40,20), c(30,20), c(30,14))
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check, csv_name="2layers")
library(gmodels)
library(class)
library(caret)
library(swirl)
library(ggplot2)
library(spatstat)
library(rpart)
library(rpart.plot)
library(stats)
library(randomForest)
library(caret)
library(kernlab)
library(RSNNS)
library(neuralnet)
source("important_functions/load_data_id.R")
source("important_functions/get_training_test_data.R")
source("important_functions/get_normalized_data.R")
source("important_functions/get_gaussian_smoothed_data.R")
source("important_functions/view_data.R")
source("important_functions/get_k_clustered_cipher_data.R")
source("important_functions/get_PCA_reduced_data.R")
################################
# load dataset all_in
# id <- load_data_id(load_full=FALSE)
id <- load_data_id(load_full=TRUE)
# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)
# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id
# split to train and test dataset
data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=50)
train_data_allin <- data_train_test_allin[[1]]
test_data_allin <- data_train_test_allin[[2]]
# splitting data disjunct
# data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
# train_data_disjunct <- data_train_test_disjunct[[1]]
# test_data_disjunct <- data_train_test_disjunct[[2]]
# decide if doing the all_in or disjunct
id_train <-train_data_allin
id_test <- test_data_allin
# id_train <-train_data_disjunct
# id_test <- test_data_disjunct
# Now the net. Note that I am splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. I am also initializing a progress bar using the plyr
#library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while.
# numbers of neurons of hidden layers to check
source("NN_functions/mlp_functions.R")
nn_sizes_check <- list(c(40,20), c(30,20), c(30,14))
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check, csv_name="2layers")
library(gmodels)
library(class)
library(caret)
library(swirl)
library(ggplot2)
library(spatstat)
library(rpart)
library(rpart.plot)
library(stats)
library(randomForest)
library(caret)
library(kernlab)
library(RSNNS)
library(neuralnet)
source("important_functions/load_data_id.R")
source("important_functions/get_training_test_data.R")
source("important_functions/get_normalized_data.R")
source("important_functions/get_gaussian_smoothed_data.R")
source("important_functions/view_data.R")
source("important_functions/get_k_clustered_cipher_data.R")
source("important_functions/get_PCA_reduced_data.R")
################################
# load dataset all_in
# id <- load_data_id(load_full=FALSE)
id <- load_data_id(load_full=TRUE)
# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)
# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id
# split to train and test dataset
data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=50)
train_data_allin <- data_train_test_allin[[1]]
test_data_allin <- data_train_test_allin[[2]]
# splitting data disjunct
# data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
# train_data_disjunct <- data_train_test_disjunct[[1]]
# test_data_disjunct <- data_train_test_disjunct[[2]]
# decide if doing the all_in or disjunct
id_train <-train_data_allin
id_test <- test_data_allin
# id_train <-train_data_disjunct
# id_test <- test_data_disjunct
# Now the net. Note that I am splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. I am also initializing a progress bar using the plyr
#library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while.
# numbers of neurons of hidden layers to check
source("NN_functions/mlp_functions.R")
nn_sizes_check <- list(c(c(80,40,20), c(60, 40, 20), c(50, 30, 14))
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check, csv_name="allin_3layers")
# Now the net. Note that I am splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. I am also initializing a progress bar using the plyr
#library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while.
# numbers of neurons of hidden layers to check
source("NN_functions/mlp_functions.R")
nn_sizes_check <- list(c(80,40,20), c(60, 40, 20), c(50, 30, 14))
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check, csv_name="allin_3layers")
