library(randomForest)
library(caret)
library(kernlab)
library(RSNNS)
library(neuralnet)
source("important_functions/load_data_id.R")
source("important_functions/get_training_test_data.R")
source("important_functions/get_normalized_data.R")
source("important_functions/get_gaussian_smoothed_data.R")
source("important_functions/view_data.R")
source("important_functions/get_k_clustered_cipher_data.R")
source("important_functions/get_PCA_reduced_data.R")
################################
# load dataset all_in
id <- load_data_id(load_full=FALSE)
# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)
# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id
# split to train and test dataset
data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=50)
train_data_allin <- data_train_test_allin[[1]]
test_data_allin <- data_train_test_allin[[2]]
# splitting data disjunct
data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
train_data_disjunct <- data_train_test_disjunct[[1]]
test_data_disjunct <- data_train_test_disjunct[[2]]
# decide if doing the all_in or disjunct
# id_train <-train_data_allin
# id_test <- test_data_allin
id_train <-train_data_disjunct
id_test <- test_data_disjunct
####################################
####### 5.2: Neural Networks #######
####################################
#######################################################################################################
## 5.2.1
## Create a matrix (as below in grey background) for the training classes. It has N rows (the same as
## the number of training data) and 10 columns (binary). The column with '1' marks the
## corresponding class as the example shown below (eg. cyper '0' is represented by '1000000000').
#######################################################################################################
source("NN_functions/mlp_functions.R")
trainingClass <- get_training_class(training_data=id_train)
################# test creation of the 1,2,3 hidden layers MLPs #################
network <- train_mlp(id_train, trainingClass, 3)
# network <- train_mlp(id_train, trainingClass, c(2, 2))
# network <- train_mlp(id_train, trainingClass, c(2, 2, 2))
################# evaluate the network
evaluate_nn(network, id_test)
library(gmodels)
library(class)
library(caret)
library(swirl)
library(ggplot2)
library(spatstat)
library(rpart)
library(rpart.plot)
library(stats)
library(randomForest)
library(caret)
library(kernlab)
library(RSNNS)
library(neuralnet)
source("important_functions/load_data_id.R")
source("important_functions/get_training_test_data.R")
source("important_functions/get_normalized_data.R")
source("important_functions/get_gaussian_smoothed_data.R")
source("important_functions/view_data.R")
source("important_functions/get_k_clustered_cipher_data.R")
source("important_functions/get_PCA_reduced_data.R")
################################
# load dataset all_in
# id <- load_data_id(load_full=FALSE)
id <- load_data_id(load_full=TRUE)
# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)
# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id
# split to train and test dataset
data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=50)
train_data_allin <- data_train_test_allin[[1]]
test_data_allin <- data_train_test_allin[[2]]
# splitting data disjunct
data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
train_data_disjunct <- data_train_test_disjunct[[1]]
test_data_disjunct <- data_train_test_disjunct[[2]]
# decide if doing the all_in or disjunct
# id_train <-train_data_allin
# id_test <- test_data_allin
id_train <-train_data_disjunct
id_test <- test_data_disjunct
library(gmodels)
library(class)
library(caret)
library(swirl)
library(ggplot2)
library(spatstat)
library(rpart)
library(rpart.plot)
library(stats)
library(randomForest)
library(caret)
library(kernlab)
library(RSNNS)
library(neuralnet)
source("important_functions/load_data_id.R")
source("important_functions/get_training_test_data.R")
source("important_functions/get_normalized_data.R")
source("important_functions/get_gaussian_smoothed_data.R")
source("important_functions/view_data.R")
source("important_functions/get_k_clustered_cipher_data.R")
source("important_functions/get_PCA_reduced_data.R")
################################
# load dataset all_in
# id <- load_data_id(load_full=FALSE)
id <- load_data_id(load_full=TRUE)
# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)
# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id
# split to train and test dataset
data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=50)
train_data_allin <- data_train_test_allin[[1]]
test_data_allin <- data_train_test_allin[[2]]
# splitting data disjunct
data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
train_data_disjunct <- data_train_test_disjunct[[1]]
test_data_disjunct <- data_train_test_disjunct[[2]]
# decide if doing the all_in or disjunct
id_train <-train_data_allin
id_test <- test_data_allin
# id_train <-train_data_disjunct
# id_test <- test_data_disjunct
####################################
####### 5.2: Neural Networks #######
####################################
#######################################################################################################
## 5.2.1
## Create a matrix (as below in grey background) for the training classes. It has N rows (the same as
## the number of training data) and 10 columns (binary). The column with '1' marks the
## corresponding class as the example shown below (eg. cyper '0' is represented by '1000000000').
#######################################################################################################
source("NN_functions/mlp_functions.R")
trainingClass <- get_training_class(training_data=id_train)
################# test creation of the 1,2,3 hidden layers MLPs #################
network <- train_mlp(id_train, trainingClass, 3)
# network <- train_mlp(id_train, trainingClass, c(2, 2))
# network <- train_mlp(id_train, trainingClass, c(2, 2, 2))
################# evaluate the network
evaluate_nn(network, id_test)
# Now the net. Note that I am splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. I am also initializing a progress bar using the plyr
#library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while.
# numbers of neurons of hidden layers to check
source("NN_functions/mlp_functions.R")
nn_sizes_check <- list(c(40,20), c(30,20), c(30,14))
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check, csv_name="2layers")
library(gmodels)
library(class)
library(caret)
library(swirl)
library(ggplot2)
library(spatstat)
library(rpart)
library(rpart.plot)
library(stats)
library(randomForest)
library(caret)
library(kernlab)
library(RSNNS)
library(neuralnet)
source("important_functions/load_data_id.R")
source("important_functions/get_training_test_data.R")
source("important_functions/get_normalized_data.R")
source("important_functions/get_gaussian_smoothed_data.R")
source("important_functions/view_data.R")
source("important_functions/get_k_clustered_cipher_data.R")
source("important_functions/get_PCA_reduced_data.R")
################################
# load dataset all_in
# id <- load_data_id(load_full=FALSE)
id <- load_data_id(load_full=TRUE)
# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)
# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id
# split to train and test dataset
data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=50)
train_data_allin <- data_train_test_allin[[1]]
test_data_allin <- data_train_test_allin[[2]]
# splitting data disjunct
# data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
# train_data_disjunct <- data_train_test_disjunct[[1]]
# test_data_disjunct <- data_train_test_disjunct[[2]]
# decide if doing the all_in or disjunct
id_train <-train_data_allin
id_test <- test_data_allin
# id_train <-train_data_disjunct
# id_test <- test_data_disjunct
# Now the net. Note that I am splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. I am also initializing a progress bar using the plyr
#library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while.
# numbers of neurons of hidden layers to check
source("NN_functions/mlp_functions.R")
nn_sizes_check <- list(c(40,20), c(30,20), c(30,14))
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check, csv_name="2layers")
library(gmodels)
library(class)
library(caret)
library(swirl)
library(ggplot2)
library(spatstat)
library(rpart)
library(rpart.plot)
library(stats)
library(randomForest)
library(caret)
library(kernlab)
library(RSNNS)
library(neuralnet)
source("important_functions/load_data_id.R")
source("important_functions/get_training_test_data.R")
source("important_functions/get_normalized_data.R")
source("important_functions/get_gaussian_smoothed_data.R")
source("important_functions/view_data.R")
source("important_functions/get_k_clustered_cipher_data.R")
source("important_functions/get_PCA_reduced_data.R")
################################
# load dataset all_in
# id <- load_data_id(load_full=FALSE)
id <- load_data_id(load_full=TRUE)
# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)
# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id
# split to train and test dataset
data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=50)
train_data_allin <- data_train_test_allin[[1]]
test_data_allin <- data_train_test_allin[[2]]
# splitting data disjunct
# data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
# train_data_disjunct <- data_train_test_disjunct[[1]]
# test_data_disjunct <- data_train_test_disjunct[[2]]
# decide if doing the all_in or disjunct
id_train <-train_data_allin
id_test <- test_data_allin
# id_train <-train_data_disjunct
# id_test <- test_data_disjunct
# Now the net. Note that I am splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. I am also initializing a progress bar using the plyr
#library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while.
# numbers of neurons of hidden layers to check
source("NN_functions/mlp_functions.R")
nn_sizes_check <- list(c(c(80,40,20), c(60, 40, 20), c(50, 30, 14))
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check, csv_name="allin_3layers")
# Now the net. Note that I am splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. I am also initializing a progress bar using the plyr
#library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while.
# numbers of neurons of hidden layers to check
source("NN_functions/mlp_functions.R")
nn_sizes_check <- list(c(80,40,20), c(60, 40, 20), c(50, 30, 14))
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check, csv_name="allin_3layers")
library(gmodels)
library(class)
library(caret)
library(swirl)
library(ggplot2)
library(spatstat)
library(rpart)
library(rpart.plot)
library(stats)
library(randomForest)
library(caret)
library(kernlab)
library(RSNNS)
library(neuralnet)
source("important_functions/load_data_id.R")
source("important_functions/get_training_test_data.R")
source("important_functions/get_normalized_data.R")
source("important_functions/get_gaussian_smoothed_data.R")
source("important_functions/view_data.R")
source("important_functions/get_k_clustered_cipher_data.R")
source("important_functions/get_PCA_reduced_data.R")
################################
# load dataset all_in
# id <- load_data_id(load_full=FALSE)
id <- load_data_id(load_full=TRUE)
# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)
# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id
# split to train and test dataset
# data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=50)
# train_data_allin <- data_train_test_allin[[1]]
# test_data_allin <- data_train_test_allin[[2]]
# splitting data disjunct
# data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
# train_data_disjunct <- data_train_test_disjunct[[1]]
# test_data_disjunct <- data_train_test_disjunct[[2]]
# decide if doing the all_in or disjunct
# id_train <-train_data_allin
# id_test <- test_data_allin
# id_train <-train_data_disjunct
# id_test <- test_data_disjunct
library(gmodels)
library(class)
library(caret)
library(swirl)
library(ggplot2)
library(spatstat)
library(rpart)
library(rpart.plot)
library(stats)
library(randomForest)
library(caret)
library(kernlab)
library(RSNNS)
library(neuralnet)
source("important_functions/load_data_id.R")
source("important_functions/get_training_test_data.R")
source("important_functions/get_normalized_data.R")
source("important_functions/get_gaussian_smoothed_data.R")
source("important_functions/view_data.R")
source("important_functions/get_k_clustered_cipher_data.R")
source("important_functions/get_PCA_reduced_data.R")
################################
# load dataset all_in
# id <- load_data_id(load_full=FALSE)
id <- load_data_id(load_full=TRUE)
# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)
# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id
# split to train and test dataset
# data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=50)
# train_data_allin <- data_train_test_allin[[1]]
# test_data_allin <- data_train_test_allin[[2]]
# splitting data disjunct
# data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
# train_data_disjunct <- data_train_test_disjunct[[1]]
# test_data_disjunct <- data_train_test_disjunct[[2]]
# decide if doing the all_in or disjunct
# id_train <-train_data_allin
# id_test <- test_data_allin
# id_train <-train_data_disjunct
# id_test <- test_data_disjunct
# Now the net. Note that I am splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. I am also initializing a progress bar using the plyr
#library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while.
# numbers of neurons of hidden layers to check
source("NN_functions/mlp_functions.R")
nn_sizes_check <- list(c(80,40,20), c(60, 40, 20), c(50, 30, 14))
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check, csv_name="3layers", do_all_in=TRUE)
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check, csv_name="3layers", do_all_in=FALSE)
dim(id)
# Now the net. Note that I am splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. I am also initializing a progress bar using the plyr
#library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while.
# numbers of neurons of hidden layers to check
source("NN_functions/mlp_functions.R")
nn_sizes_check <- list(c(1,1), c(2,2))
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check, csv_name="2layers_test", do_all_in=TRUE)
# Now the net. Note that I am splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. I am also initializing a progress bar using the plyr
#library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while.
# numbers of neurons of hidden layers to check
source("NN_functions/mlp_functions.R")
nn_sizes_check <- list(c(1,1), c(2,2))
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check, csv_name="2layers_test", do_all_in=TRUE)
View(id_norm)
View(id_norm)
library(gmodels)
library(class)
library(caret)
library(swirl)
library(ggplot2)
library(spatstat)
library(rpart)
library(rpart.plot)
library(stats)
library(randomForest)
library(caret)
library(kernlab)
library(RSNNS)
library(neuralnet)
source("important_functions/load_data_id.R")
source("important_functions/get_training_test_data.R")
source("important_functions/get_normalized_data.R")
source("important_functions/get_gaussian_smoothed_data.R")
source("important_functions/view_data.R")
source("important_functions/get_k_clustered_cipher_data.R")
source("important_functions/get_PCA_reduced_data.R")
################################
# load dataset all_in
# id <- load_data_id(load_full=FALSE)
id <- load_data_id(load_full=TRUE)
# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)
# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id
# split to train and test dataset
# data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=50)
# train_data_allin <- data_train_test_allin[[1]]
# test_data_allin <- data_train_test_allin[[2]]
# splitting data disjunct
# data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
# train_data_disjunct <- data_train_test_disjunct[[1]]
# test_data_disjunct <- data_train_test_disjunct[[2]]
# decide if doing the all_in or disjunct
# id_train <-train_data_allin
# id_test <- test_data_allin
# id_train <-train_data_disjunct
# id_test <- test_data_disjunct
# Now the net. Note that I am splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. I am also initializing a progress bar using the plyr
#library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while.
# numbers of neurons of hidden layers to check
source("NN_functions/mlp_functions.R")
nn_sizes_check <- list(c(1,1), c(2,2))
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check, csv_name="2layers_test", do_all_in=TRUE)
folds <- createFolds(id_use[,1], k = 10)
folds <- createFolds(id_use[,1], k = 10)
View(folds)
folds[1]
folds <- create_folds_disjunct(data[,1], k = 10)
list( 1:2000, 2001:4000, 40001:6000, 6001:8000, 8001:1000, 10001:12000, 12001:14000, 14001:16000, 16001:18000, 18001:20000 )
list_ids <- list( 1:2000, 2001:4000, 40001:6000, 6001:8000, 8001:1000, 10001:12000, 12001:14000, 14001:16000, 16001:18000, 18001:20000 )
View(list_ids)
View(folds)
list_ids <- list( 1:2000, 2001:4000, 4001:6000, 6001:8000, 8001:1000, 10001:12000, 12001:14000, 14001:16000, 16001:18000, 18001:20000 )
list_ids <- list( 1:2000, 2001:4000, 4001:6000, 6001:8000, 8001:10000, 10001:12000, 12001:14000, 14001:16000, 16001:18000, 18001:20000 )
list_ids <- list( 1:2000, 2001:4000, 4001:6000, 6001:8000, 8001:10000, 10001:12000, 12001:14000, 14001:16000, 16001:18000, 18001:20000 )
View(list_ids)
# Now the net. Note that I am splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. I am also initializing a progress bar using the plyr
#library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while.
# numbers of neurons of hidden layers to check
source("NN_functions/mlp_functions.R")
nn_sizes_check <- list(c(1,1))
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check, csv_name="2layers_test", do_all_in=TRUE)
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check, csv_name="2layers_test", do_all_in=FALSE)
library(gmodels)
library(class)
library(caret)
library(swirl)
library(ggplot2)
library(spatstat)
library(rpart)
library(rpart.plot)
library(stats)
library(randomForest)
library(caret)
library(kernlab)
library(RSNNS)
library(neuralnet)
source("important_functions/load_data_id.R")
source("important_functions/get_training_test_data.R")
source("important_functions/get_normalized_data.R")
source("important_functions/get_gaussian_smoothed_data.R")
source("important_functions/view_data.R")
source("important_functions/get_k_clustered_cipher_data.R")
source("important_functions/get_PCA_reduced_data.R")
################################
# load dataset all_in
# id <- load_data_id(load_full=FALSE)
id <- load_data_id(load_full=TRUE)
# preprocessing options
# id_smooth <- get_gaussian_smoothed_data(dataset=id, smooth_sigma=0.05)
id_norm <- get_normalized_data(id)
# which of the preprocessing to use
# id_use <- id_smooth
id_use <- id_norm
# id_use <- id
# split to train and test dataset
# data_train_test_allin <- get_training_test_data_allin(data=id_use, training_percent=50)
# train_data_allin <- data_train_test_allin[[1]]
# test_data_allin <- data_train_test_allin[[2]]
# splitting data disjunct
# data_train_test_disjunct <- get_training_test_data_disjunct(data=id_use)
# train_data_disjunct <- data_train_test_disjunct[[1]]
# test_data_disjunct <- data_train_test_disjunct[[2]]
# decide if doing the all_in or disjunct
# id_train <-train_data_allin
# id_test <- test_data_allin
# id_train <-train_data_disjunct
# id_test <- test_data_disjunct
# Now the net. Note that I am splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. I am also initializing a progress bar using the plyr
#library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while.
# numbers of neurons of hidden layers to check
source("NN_functions/mlp_functions.R")
# nn_sizes_check <- list(c(1,1), c(2,1), c(2,2))
nn_sizes_check_2 <- list(c(40,20), c(30,20), c(30,14))
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check_2, csv_name="2layers", do_all_in=TRUE)
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check_2, csv_name="2layers", do_all_in=FALSE)
nn_sizes_check_3 <- list(c(80,40,20), c(60, 40, 20), c(50, 30, 14))
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check_3, csv_name="3layers", do_all_in=TRUE)
data_mlp_cv <- generate_cross_validation_mlp(data=id_use, nn_sizes_check=nn_sizes_check_3, csv_name="3layers", do_all_in=FALSE)
