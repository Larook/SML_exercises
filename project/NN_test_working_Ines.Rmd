---
title: "SVM_NeuralNets"
output: pdf_document
---

```{r}
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(rpart.plot) #SKAL INSTALLERES
library(stats)
library(randomForest)
library(caret)
library(kernlab)
library(RSNNS)
```

```{r}
#Load the dataset
# load("idList-cornered-100-2021.Rdata")
# load("/data/idList-cornered-100-2021.Rdata")
load("data/idList-co-100.Rdata")

id <- do.call(rbind, idList[])
id <- as.data.frame(id)
id[,1] <- factor(id[,1])

items_per_person = nrow(id) / length(idList)
id_train <- id[1:(items_per_person*9),]
id_test <- id[(items_per_person*9 + 1):(items_per_person*13),]
```


```{r}
#####################
## Exercise 5.1 - SVM
#####################

#########
## 5.1.1
#########

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}


id <- do.call(rbind, idList[1:2])
id <- as.data.frame(id)
id[,1] <- factor(id[,1])

id_sn <- as.data.frame(lapply(id[-1], normalize))

## 50/50 split of dataset
smp_size <- floor(0.5 * nrow(id))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(id)), size = smp_size)

train <- id[train_ind, ]
test <- id[-train_ind, ]


# ...................... TRY DIFFERENT C VALUES ................................................

#classifier_rbf <-ksvm(V1~ ., data = train, kernel = "vanilladot", C = 1)
#classifier_rbf <-ksvm(V1~., data = train, kernel = "rbfdot", kpar=list(sigma=0.05), C = 1)
classifier_rbf <-ksvm(V1~ ., data = id, kernel = "polydot", C = 1)


datasetTest <- predict(classifier_rbf,test)

#table(datasetTest, test[,1])
plot(datasetTest, test[,1])


```

```{r}
####################################
####### 5.2: Neural Networks ####### 
####################################

#######################################################################################################
## 5.2.1
## Create a matrix (as below in grey background) for the training classes. It has N rows (the same as 
## the number of training data) and 10 columns (binary). The column with '1' marks the 
## corresponding class as the example shown below (eg. cyper '0' is represented by '1000000000').
#######################################################################################################

lev <- levels(id_train$V1) # Number of classes

# Create a list probabilities, for all labels
nnTrainingClass <- matrix(nrow = length(id_train$V1), ncol = 10, data = 0) 

for(i in 1:length(id_train$V1)) { # Set probabilities to one for matching class
  matchList <- match(lev,toString(id_train$V1[i]))
  matchList[is.na(matchList)] <- 0
  nnTrainingClass[i,] <- matchList
}
trainingClass <- as.data.frame(nnTrainingClass)

```

```{r}
############################################################################################
## 5.2.2
## Train a neural network with N inputs and 10 outputs, based on the modified training data.
############################################################################################


train_mlp <- function(training_data, training_classes, size) {
  
#network <- mlp(id[,-1], trainingClass, size = c(2,2,2), maxit = 2, hiddenActFunc = "Act_TanH", learnFunc="Std_Backpropagation", learnFuncParams = c(0.01,0))

  network <- mlp(id_train[,-1], training_classes, size = size, maxit = 300, hiddenActFunc = "Act_TanH", learnFunc="Std_Backpropagation", learnFuncParams = c(0.01,0))
  
  plotIterativeError(network)
  
  network$IterativeFitError[300]
  
  return(network)
}

network <- train_mlp(id_train, trainingClass, c(60,40,20))

```




