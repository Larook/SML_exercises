---
title: "Exercise_1_KNN"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gmodels)
library(class)
library(caret)
library(swirl)
```

## Exercise 2

## Excercise 2.1 Principal Component Analysis (PCA)
Perform a PCA on the data for the “all persons in” and “disjunct” data set.

PCA loading vector - the directions in feature space along which the data vary the most
PCA scores - projections along these directions


### 2.1.1 Show the standard deviation ( From prcomp Eigenvalues ), the proportion of variance and the cumulative sum of variance of the principal components. (In the report the first 10-20 principal components, should be sufficient to illustrate the tendencies.)


``` {r PCA}
print_plots_pca <- function(data){
  # help https://www.datacamp.com/community/tutorials/pca-analysis-r
  df_pca <- prcomp(x=data[,c(2:235)])
  pca_sum <- summary(df_pca)
  print(pca_sum)
  
  # Zouchi mentioned using squaring of sdev!!!
  
  #Show the standard deviation ( From prcomp Eigenvalues )
  plot(df_pca$sdev, main="df_pca$sdev ")
  # what does that mean?
  variance = df_pca$sdev^2 # since variance = st.dev^2
  plot(variance, main="variance of loadings")
  
  # proportion of variance
  prop_of_variance <- df_pca$sdev^2/sum(df_pca$sdev^2)
  plot(prop_of_variance, main="proportion of varpiance")
  
  #cumulative sum of variance of the principal components
  #fuck it at this point I think using the summary was ok - exactly what he wanted to see - if we don't include code its all he wants
  df_pca$eig
  
  
  plot(, main="cumulative sum of variance of the principal component")
  
  # my previous tries before remarks from Zouchi
  print(pca_sum$importance[,1:20])
  pca_std_variance <- pca_sum$importance[1,1:20]
  plot(pca_std_variance, main="standard deviation of first 20 PCAs")
  
  pca_proportion_of_variance <- pca_sum$importance[2,1:20]
  plot(pca_proportion_of_variance, main="proportion of variance of first 20 PCAs")
  
  pca_cumulative_proportion <- pca_sum$importance[3,1:20]
  plot(pca_cumulative_proportion, main="cumulative proportion first 20 PCAs")
}


# All persons in
load("../data/idList-co-100.Rdata")
id <- do.call(rbind, idList[1:10])
id <- as.data.frame(id)
id$V1 <- factor(id$V1)
# id - huge dataframe with everyone as TRAINING

set.seed(423)
shuffled_df <- id[sample(nrow(id)),]
print_plots_pca(shuffled_df)

# disjunt data
#id <- idList[[1]]
#shuffled_df <- id[sample(nrow(id)),]
#print_plots_pca(shuffled_df)

```

## 2.1.2 Show the performance of selecting enough principal components to represent 80%, 90%, 95%, 99% of the accumulated variance. For each test vary “k” in kNN, try 3 reasonable values.
``` {r performance_knn_with_pca}
# Zouchi told sth to use prcomp $x if I remember correctly - we really need to read the documentation
# how to get the data with only few pcs?

# function that prints the image
get_img <- function(df, sample_no){
  # get matrix of number from dataset
  # df-dataset; sample_no - which sample to return the matrix
  img <- data.matrix(df[sample_no,2:ncol(df)])
  matrix(img, nrow = 18, ncol = 18, byrow = FALSE)
}

# function that returns the wanted PCs
get_pcs_covering_variance <-function(data, searched_variance){
  pca <- prcomp(data[,c(2:235)], center = TRUE,scale. = TRUE)
  pca_sum <- summary(pca)
  
  i <- 1 # keep track of the principle component number
  cumul_prop = pca_sum$importance[3, i] # get cumulative proportion of current PC
  print(paste0("searched_accum_var=",searched_accum_var))
  print(paste0("i=",i, " Cumulative Proportion=",cumul_prop))
  while(cumul_prop*100 <  searched_accum_var){
    i <- i + 1
    cumul_prop = pca_sum$importance[3, i] # get cumulative proportion of current PC
    print(paste0("i=",i, " Cumulative Proportion=",cumul_prop))
  }
  searched_pcs = pca_sum$importance[, seq(1, i, 1)]
  print(paste0("To get the ", searched_accum_var, "% variance, wee need first ", i, " principal components"))
  #print(searched_pcs)
  df_pca$x[, seq(1, i, 1)]
}



load("../data/id100.Rda")
shuffled_df <- id[sample(nrow(id)),]

df_pca <- prcomp(id[,c(2:235)], center = TRUE,scale. = TRUE)
pca_sum <- summary(df_pca)
#print(pca_sum$x)

# how to get the components that give us the % of variance?

#accumulated_variance_v <- c(80, 90, 95, 99)
accumulated_variance_v <- c(80)
for (searched_accum_var in accumulated_variance_v){
  wanted_pcs <- get_pcs_covering_variance(id, searched_accum_var)
  print(wanted_pcs)
  print(paste0("function returns data with dimensions", dim(wanted_pcs)))
  # Now the questions is how to understand this data?
  
  # let's try and print first pca as image
  # visualize image
  image(get_img(wanted_pcs, 1), col = gray(0:100/100)) # looks most probably bad - should in any way resemble any number - either understanding of pc is wrong or getting an image of cipher
  
  #try to show any number
  image(get_img(shuffled_df, 13), col = gray(0:100/100) )
  
  #ok it seems like we have to rotate it additionally-there is a code for that I guess in the pdf of exercise
  #still it seems like the PCs gotten are wrong, but the dimensions seemed okish
}
```









# Below are only leftovers from exc 1
```{r}
load("../data/id100.Rda")
set.seed(423)

k_error_avg_vec <- c()
timing_now_avg_vec <- c()
ks <- c( 19)

for (k_now in ks ){
  #print(paste0("k_now = ", k_now))
  k_error <- 0
  k_error_avg <- 0
  
  timing_now_avg <- 0
  for (i in 1:10) {
    # shuffle dataset
    shuffled_df <- id[sample(nrow(id)),]
    
    # split 10/90 for training and testing data
    test_df <- shuffled_df[1:200,]  # shuffled_df[1:360,]
    train_df <- shuffled_df[201:400,] # shuffled_df[361:400,]
    
    # get the labels - supervision part
    id_train_labels <- train_df[,1]
    id_test_labels <- test_df[,1]
    
    # check the time
    t_start <- Sys.time()
    # get the prediction
    numbers_test_pred <- knn(train = train_df[,c(2:325)], test = test_df[,c(2:325)], cl = id_train_labels, k=k_now)
    timing_now <- (Sys.time() - t_start ) * 1000 #ms
    
    k_error <- k_error +mean(id_test_labels != numbers_test_pred)
    timing_now_avg <- timing_now_avg + timing_now

  }
  # get summary of one k
  k_error_avg <- k_error / 10
  k_error_avg_vec <- c(k_error_avg_vec, k_error_avg)
  
  timing_now_avg <- timing_now_avg / 10
  timing_now_avg_vec <- c(timing_now_avg_vec, timing_now_avg) 
  
  print(paste0("k_now = ", k_now, " k_error_avg = ", k_error_avg, " timing_now_avg=", timing_now_avg ))
}
#plot(ks, k_error_avg_vec, type="o", ylab="misclassification error")
#plot(ks, timing_now_avg_vec, type="o", ylab="average execution time [ms]")

```


# one guy mentioned
I created a function for making perato plots for pca: https://pastebin.com/KcE7BcNS


``` {r case A}
#DONT KNOW WHY IT GIVES ERRORS!

load("data/idList-co-100.Rdata")
id <- do.call(rbind, idList[1:10])
id <- as.data.frame(id)
id$V1 <- factor(id$V1)

# id - huge dataframe with everyone as TRAINING
train_df <- id
id_train_labels <- train_df[,1]

person_error_v <- c()
# individually test sets for each person
for (person_df in idList){
  print("new_person")
  test_df <- person_df
  id_test_labels <- person_df[,1]
  
  # get the prediction
  numbers_test_pred <- knn(train = train_df[,c(2:325)], test = test_df[,c(2:325)],cl = id_train_labels, k=15)
  
  person_error <- mean(id_test_labels != numbers_test_pred)
  person_error_v <- c(person_error_v, person_error)
} 

#print(paste0("dim(person_error_v) = ", dim(person_error_v)))
plot(1:length(idList), person_error_v, type="o", ylab="misclassification error", xlab="person number", main="case A")

```