---
title: "Exercise_1_KNN"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gmodels)
library(class)
library(caret)
library(swirl)
```

## Exercise 2
#official saviour https://github.com/LennartOlsen/pca-digits/blob/master/main.r
## Excercise 2.1 Principal Component Analysis (PCA)
Perform a PCA on the data for the “all persons in” and “disjunct” data set.

PCA loading vector - the directions in feature space along which the data vary the most
PCA scores - projections along these directions


### 2.1.1 Show the standard deviation ( From prcomp Eigenvalues ), the proportion of variance and the cumulative sum of variance of the principal components. (In the report the first 10-20 principal components, should be sufficient to illustrate the tendencies.)


``` {r PCA}
print_plots_pca <- function(data){
  # help https://www.datacamp.com/community/tutorials/pca-analysis-r
  df_pca <- prcomp(x=data[,(2:ncol(data))], scale = TRUE, center = TRUE)
  pca_sum <- summary(df_pca)
  print(pca_sum)
  

  # my previous tries before remarks from Zouchi
  print(pca_sum$importance[,1:30])
  pca_std_variance <- pca_sum$importance[1,1:30]
  plot(x=c(1:30), y=pca_std_variance, xlab="principal components", ylab="standard deviation", main="standard deviation of first 30 PCAs")
  
  pca_proportion_of_variance <- pca_sum$importance[2,1:30]
  plot(x=c(1:30), y=pca_proportion_of_variance, xlab="principal components", ylab="proportion of variance", main="proportion of variance of first 30 PCAs")

  pca_cumulative_proportion <- pca_sum$importance[3,1:30]
  plot(x=c(1:30), y=pca_cumulative_proportion, xlab="principal components", ylab="cumulative proportion" , main="cumulative proportion first 30 PCAs")
}


# All persons in
# load("../data/idList-co-100.Rdata")
load("../data/idList-cornered-100-2021.Rdata") 
id <- do.call(rbind, idList[1:10])
id <- as.data.frame(id)
id$V1 <- factor(id$V1)
# id - huge dataframe with everyone as TRAINING

set.seed(423)
shuffled_df <- id[sample(nrow(id)),]
print_plots_pca(shuffled_df)

# disjunt data
#id <- idList[[1]]
#shuffled_df <- id[sample(nrow(id)),]
#print_plots_pca(shuffled_df)

```

## 2.1.2 Show the performance of selecting enough principal components to represent 80%, 90%, 95%, 99% of the accumulated variance. For each test vary “k” in kNN, try 3 reasonable values.

``` {r VERY_IMPORTANT_FUNCTIONS}
# Zouchi told sth to use prcomp $x if I remember correctly - we really need to read the documentation
# how to get the data with only few pcs?
#VERY IMPORTANT FUNCTIONS

# get accuracy of the labels in percents
get_accuracy <- function(predicted_labels, test_labels){
  # accuracy(table(numbers_test_pred,id_test_labels))
  tab <- table(predicted_labels, test_labels)
  sum(diag(tab)/(sum(rowSums(tab)))) * 100
}

# function that prints the image
get_img <- function(df, sample_no){
  # get matrix of number from dataset
  # df-dataset; sample_no - which sample to return the matrix
  img <- data.matrix(df[sample_no,2:ncol(df)])
  matrix(img, nrow = 18, ncol = 18, byrow = FALSE)
}

rotate <- function(x) t(apply(x, 2, rev))

# prints rotated image (we use this)
get_img_rot <- function(dataset, id_no){
  # modified code that we got in exercise, put only dataset and id of number
  id_mat <- data.matrix(dataset, rownames.force = NA)
  # get digit from id
  rotated <- c(id_mat[id_no, 2:ncol(id_mat)])
  rotated <- ((rotated - min(rotated)) / (max(rotated) - min(rotated)))
  image <- matrix(rotated,nrow = 18,ncol = 18, byrow = FALSE)
  image <- rotate(image)
}

# get the maximum needed number for getting the searched variance 
get_pcs_for_accum_variance <-function(data, searched_variance){
  pca <- prcomp(data[,(2:ncol(data))], center = TRUE,scale. = TRUE)
  pca_sum <- summary(pca)
  
  i <- 1 # keep track of the principle component number
  cumul_prop = pca_sum$importance[3, i] # get cumulative proportion of current PC
  #print(paste0("searched_accum_var=",searched_accum_var))
  #print(paste0("i=",i, " Cumulative Proportion=",cumul_prop))
  while(cumul_prop*100 <  searched_variance){
    i <- i + 1
    cumul_prop = pca_sum$importance[3, i] # get cumulative proportion of current PC
    #print(paste0("i=",i, " Cumulative Proportion=",cumul_prop))
  }
  print(paste0("To get the ", searched_variance, "% variance, wee need first ", i, " principal components"))
  i
}


# get reconstruction of pca
get_reconstructed_pca <- function(dataset){
    restr <- dataset$x %*% t(dataset$rotation)
    # unscale and uncenter the data
    if(dataset$scale != FALSE){
      restr <- scale(restr, center = FALSE , scale=1/dataset$scale)
    }
    if(all(dataset$center != FALSE)){
      restr <- scale(restr, center = -1 * dataset$center, scale=FALSE)
    }
   restr
}

# show the cipher gotten from reconstruction
show_pca_reconstruction <- function(dataset, searched_variance){
  
  limit_pca_rank <- get_pcs_for_accum_variance(dataset, searched_accum_var)# how to get the components that give us the % of variance?
  wanted_pcs <- prcomp(dataset[,(2:ncol(dataset))], center = TRUE,scale. = TRUE, rank. = limit_pca_rank)
  #print(wanted_pcs)
  #print(paste0("function returns data with dimensions", dim(wanted_pcs)))

  restr <- get_reconstructed_pca(wanted_pcs)
  image(get_img_rot(restr, cipher_index), col = gray(0:100/100) )
} 


library(ggplot2)
knn_get_pca_plot_data <- function(test_data, test_labels, train_data, train_labels, repetitions = 10, k, searched_accum_var){
      
    test_time_v <- c()
    test_accuracy_v <- c()
    train_accuracy_v <- c()
    
    for (rep in c(1:repetitions)){
      t_start <- as.numeric(proc.time())
      # get the prediction
      numbers_test_pred <- knn(train = train_data, test = test_data, cl = train_labels, k=k)
      t_end <- as.numeric(proc.time())
      delta_t <- (t_end -  t_start)[3]  # seconds
      test_time_v <- c(test_time_v, delta_t)
      #print(test_time_v)
      
      # see the performance on testing data
      k_accuracy_test <-get_accuracy(test_labels, numbers_test_pred)
      test_accuracy_v <- c(test_accuracy_v, k_accuracy_test)
      
      # check the performance on testing data
      numbers_train_pred <- knn(train = train_data, test = train_data, cl = train_labels, k=k)
      k_accuracy_train <-get_accuracy(train_labels, numbers_train_pred)
      train_accuracy_v <- c(train_accuracy_v, k_accuracy_train)
    }
    
    return(list("searched_accum_var"=searched_accum_var, "k"=k,"test_time_v"=test_time_v, "test_accuracy_v"=test_accuracy_v, "train_accuracy_v"=train_accuracy_v))
}

# Plot all the results given the data
plot_results <- function(plot_data_ks_l){
  ks <- c()
  exec_time_means <- c()
  exec_time_stds <- c()
  
  test_accuracy_means <- c()
  test_accuracy_stds <- c()
  
  train_accuracy_means <- c()
  train_accuracy_stds <- c()
    
  for (plot_data_k in plot_data_ks_l){
    #print(plot_data_k$k)
    ks <- c(ks, plot_data_k$k)
    exec_time_means <- c(exec_time_means, mean(plot_data_k$test_time_v))
    exec_time_stds <- c(exec_time_stds, sd(plot_data_k$test_time_v))
    
    test_accuracy_means <- c(test_accuracy_means, mean(plot_data_k$test_accuracy_v))
    test_accuracy_stds <- c(test_accuracy_stds, sd(plot_data_k$test_accuracy_v))
    
    train_accuracy_means <- c(train_accuracy_means, mean(plot_data_k$train_accuracy_v))
    train_accuracy_stds <- c(train_accuracy_stds, sd(plot_data_k$train_accuracy_v))
  }
  df_plots <-data.frame(ks= ks, exec_time_means=exec_time_means, test_accuracy_stds=test_accuracy_stds)
  #print(head(df_plots))
  
  main <- paste("searched accummulated variance = ", plot_data_k$searched_accum_var, "%")
  
  g_time <- ggplot(df_plots, aes(x=ks)) + 
    geom_line(aes(y=exec_time_means)) + 
    geom_ribbon(aes(ymax=exec_time_means+exec_time_stds, ymin=exec_time_means-exec_time_stds), fill="pink", alpha=.5) +
    labs(title=main, 
         subtitle="execution time knn", 
         caption="Source: abcd", 
         y="time [ms]",
         x="k-number of nearest neighbours") 
  
  colors <- c("test data" = "darkgreen", "train data" = "darkblue" )
  g_accuracy <- ggplot(df_plots, aes(x=ks)) + 
    geom_line(aes(y=test_accuracy_means, color="test data")) + 
    geom_ribbon(aes(ymax=test_accuracy_means+test_accuracy_stds, ymin=test_accuracy_means-test_accuracy_stds), fill="green", alpha=.5) +
    geom_line(aes(y=train_accuracy_means, color="train data")) + 
    geom_ribbon(aes(ymax=train_accuracy_means+train_accuracy_stds, ymin=train_accuracy_means-train_accuracy_stds), fill="blue", alpha=.5) +
  labs(title=main, 
         subtitle="accuracy comparison", 
         caption="Source: abcd", 
         y="accuracy [%]",
         x="k-number of nearest neighbours",
        color = "Legend") +
    scale_color_manual(values = colors)
  
  list_of_plots <- list(g_time, g_accuracy)
  
  return(list_of_plots)
}

# basically function does the whole exercise
get_plots_exc_2_1_all_in <- function(id_dataset, ks, accumulated_variance_v){
  list_of_plots <- list()
  set.seed(423)
  
  shuffled_df <- id_dataset[sample(nrow(id_dataset)),]
  
  # split 50/50
  test_rows_id <- 1:(nrow(shuffled_df)/2)
  train_rows_id <- ((nrow(shuffled_df)/2) + 1) :nrow(shuffled_df)
  
  
  
  for (searched_accum_var in accumulated_variance_v){
    # how to get the components that give us the % of variance?
    limit_pca_rank <- get_pcs_for_accum_variance(shuffled_df, searched_accum_var)
    
    # get pcs
    wanted_pcs <- prcomp(shuffled_df[,(2:ncol(shuffled_df))], center = TRUE,scale. = TRUE, rank. = limit_pca_rank)
    plot_data_ks_l <- list()
    for (k_now in ks){
  
      test_df <- wanted_pcs$x[test_rows_id,]  # HERE ERROR FOR DISJUNCT - can't operate on the ids -> write just separate function
      train_df <- wanted_pcs$x[train_rows_id,]
      
      # get the labels - supervision part
      id_test_labels <- shuffled_df[test_rows_id, 1]
      id_train_labels <- shuffled_df[train_rows_id, 1]
      
      plot_data <- knn_get_pca_plot_data(test_data=test_df, test_labels=id_test_labels, train_data=train_df, train_labels=id_train_labels, repetitions=10, k=k_now, searched_accum_var=searched_accum_var)
      #print(plot_data)
      plot_data_ks_l <- append(plot_data_ks_l, list(plot_data))
      
    }
    
    list_of_plots <- append(list_of_plots, plot_results(plot_data_ks_l))
    # print(list_of_plots[1])
    # print(list_of_plots[2])
    #show_pca_reconstruction(shuffled_df, searched_accum_var)
    return(list_of_plots)
  }
}

# SINGLE PERSON DATA
load("../data/id100.Rda")
set.seed(423)
shuffled_df <- id[sample(nrow(id)),]  # or maybe just dont shuffle? shuffled_df <- id

ks <- c(15, 35, 75, 150) #, 75) # ks <- c(5,9,13)
#ks <- c(2:50) #, 75) # ks <- c(5,9,13)

df_pca <- prcomp(x=shuffled_df[,(2:ncol(shuffled_df))], scale = TRUE, center = TRUE)
#df_pca <- prcomp(id[,(2:ncol(id))], center = TRUE,scale. = TRUE)
pca_sum <- summary(df_pca)
#print(pca_sum$x)
# ADD LABELS TO DF_PCA

# basically function does the whole exercise
#TODO: shuffle before PCA
get_plots_exc_2_1_disjunct <- function(id_dataset, ks, accumulated_variance_v){
  set.seed(423)
  # ks <- c(49,59,69,79) #c(35, 75) #, 75) # ks <- c(5,9,13)
  # accumulated_variance_v <- c(80, 90, 95, 99)
  list_of_plots <- list()
  shuffled_df_truly <- id_dataset[sample(nrow(id_dataset)),]
  shuffled_df <- id_dataset
  
  # split 50/50 in terms of people
  rows_per_digit <- 200
  rows_per_person <- 10 * rows_per_digit
  
  train_data_limit <- floor(nrow(id_dataset)/2)

  test_rows_id <- 1:(train_data_limit)
  train_rows_id <- (train_data_limit + 1) : nrow(id_dataset)

  for (searched_accum_var in accumulated_variance_v){
    # how to get the components that give us the % of variance?
    limit_pca_rank <- get_pcs_for_accum_variance(shuffled_df, searched_accum_var)
    
    # get pcs
    wanted_pcs <- prcomp(shuffled_df[,(2:ncol(shuffled_df))], center = TRUE,scale. = TRUE, rank. = limit_pca_rank)
    
    plot_data_ks_l <- list()
    for (k_now in ks){
  
      test_df <- as.data.frame(wanted_pcs$x[test_rows_id,]) 
      train_df <- as.data.frame(wanted_pcs$x[train_rows_id,])
      
      # get the labels - supervision part
      id_test_labels <- shuffled_df[test_rows_id, 1]
      id_train_labels <- shuffled_df[train_rows_id, 1]
      
      # join labels with data - labels go as the last column
      test_df$label <- id_test_labels 
      train_df$label <- id_train_labels 
      
      # get the labels as the first column
      test_df <- test_df[,c(ncol(test_df),1:(ncol(test_df)-1))]
      train_df <- train_df[,c(ncol(train_df),1:(ncol(train_df)-1))]
      
      # shuffle joint data
      #test_df <- test_df[sample(nrow(test_df)),]
      #train_df <- train_df[sample(nrow(train_df)),]
      
      plot_data <- knn_get_pca_plot_data(test_data=test_df[2:ncol(test_df)], test_labels=test_df$label, train_data=train_df[2:ncol(test_df)], train_labels=train_df$label, repetitions=10, k=k_now, searched_accum_var=searched_accum_var)
      #print(plot_data)
      plot_data_ks_l <- append(plot_data_ks_l, list(plot_data))
      
    }
    
    # list_of_plots <- plot_results(plot_data_ks_l)
    # print(list_of_plots[1])
    # print(list_of_plots[2])
    #show_pca_reconstruction(shuffled_df, searched_accum_var)
    list_of_plots <- append(list_of_plots, plot_results(plot_data_ks_l))
    return(list_of_plots)
  }
}
```


``` {r disjunct}

# Disjunct
load("../data/idList-cornered-100-2021.Rdata") 
id <- do.call(rbind, idList[1:10])
id <- as.data.frame(id)
id$V1 <- factor(id$V1)
# id - huge dataframe with everyone

ks <- c(39,49,59)#,69,79) #c(35, 75) #, 75) # ks <- c(5,9,13)
accumulated_variance_v <- c(80, 90)#, 95, 99)
list_of_plots_disjunct <- get_plots_exc_2_1_disjunct(id_dataset=id, ks=ks, accumulated_variance_v=accumulated_variance_v)
print(list_of_plots_disjunct[1])
print(list_of_plots_disjunct[2])
```



``` {r all persons in}
# All persons in
load("../data/idList-cornered-100-2021.Rdata") 
id <- do.call(rbind, idList[1:10])
id <- as.data.frame(id)
id$V1 <- factor(id$V1)
# id - huge dataframe with everyone as TRAINING

ks <- c(39,49,59) #c(35, 75) #, 75) # ks <- c(5,9,13)
accumulated_variance_v <- c(80, 90, 95, 99)
list_of_plots_all_in <- get_plots_exc_2_1_all_in(id_dataset=id, ks=ks, accumulated_variance_v=accumulated_variance_v)
print(list_of_plots_all_in[1])
print(list_of_plots_all_in[2])
```




``` {r single person data}

# SINGLE PERSON DATA
load("../data/id100.Rda")
get_plots_exc_2_1_all_in(id_dataset=id)

```


## 2.1.3 Measure run times for the prediction step of the kNN-classifier with PCA based dimensionality reduction. How does the feature vector dimensionality effect performance?



## 2.1.4 Interpret the results.
Shouldn't it be answered by the plots?

