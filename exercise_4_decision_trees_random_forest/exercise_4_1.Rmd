---
title: "Exercise_1_KNN"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gmodels)
library(class)
library(caret)
library(swirl)
```

# 4.1 - Decision Trees:
## 4.1.1 - Compute the optimal decision point for the first 5 PCAs of a dataset (e.g. a single person) and compute the information gain associated to it (plot 5 graphs, one for each component, and show the highest information gain). See slides for how to compute information gain.

``` {r PCA}
# define the function for normalization
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# All persons in
load("../data/idList-co-100.Rdata")
id <- do.call(rbind, idList[1:2])  # only one person
id <- as.data.frame(id)
id$V1 <- factor(id$V1)
# id - huge dataframe with everyone as TRAINING

set.seed(423)
shuffled_df <- id[sample(nrow(id)),]
df_pca <- prcomp(x=shuffled_df[,(2:ncol(shuffled_df))], scale = TRUE, center = TRUE, rank.=5)
```
compute the information gain associated to it (plot 5 graphs, one for each component, and show the highest information gain). See slides for how to compute information gain.
```{r info_gain}
# define the function for normalization
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# prepare data
load("../data/idList-co-100.Rdata")
id <- do.call(rbind, idList[1:2])
id <- as.data.frame(id)
id[,1] <- factor(id[,1])

id_sn <- as.data.frame(lapply(id[-1], normalize))

################################################################# Exercise 4.1.1 ################################################################# 

# define the function to compute entropy
entropy <- function(S) { # Function to calculate entropy as in the slides
  fullSum <- 0
  for( i in (0:9) ) { 
    if( nrow(S) > 0 ) # Make sure that there is something in the list
      { pi <- nrow(S[ S[,1] == i , ]) / nrow(S) }else{pi <- 0} 
    if(pi > 0 ){
    fullSum <- fullSum - pi * log2(pi)
    }
  }
  return(fullSum)
}

id_pca <- prcomp(id_sn, center = TRUE, scale. = TRUE) 

id_pca_first_5 <- id_pca$x[, 1:5] # Take first five scores
id_pca_first_5 <- data.frame(id_pca_first_5) # Make it a data frame

# Calculate starting entropy
entBefore <- entropy(id) 
par(mfrow=c(2,3)) 
for( img in (1:5)){
  entList <- c() 
  xList <- c()  
  
  Pts <- seq(min(id_pca_first_5[ ,img]), max(id_pca_first_5[ ,img]), length.out=200)
  for( splitP in (1:200) ){
    S1 <- id[ id_pca_first_5[ ,img] < Pts[splitP], ]   # Perform splits
    S2 <- id[ id_pca_first_5[ ,img] >= Pts[splitP], ]
    s1 <- nrow(S1) 
    s2 <- nrow(S2)
    xList[splitP] <- Pts[splitP]
    ent <- ( s1 * entropy(S1) )/(s1 + s2) + ( s2 * entropy(S2) )/(s1 + s2)  # Calculate entropy
    entList[splitP] <-  entBefore - ent 	# Information gain is calculated 
  }
  plot(xList, entList, xlab="sequence from min to max value of this PC",ylab=paste("Information gain of PC ", img) )
}

```
## Understanding the information gain plots
x - sequence of min and max values of Principal Components (prcomp.x), we use it to find the best threshold to do classification
y - for every point of x.axis we are calculating the information gain, that we achieve with comparing the entropy before and after PCA decomposition.

we split data based on min and max values of PCA as thresholds-> split rows (means digits)
for every PCA 
we compare then the entropies to choose the optimal point which gives the highest information gain.

So now for every PCA we have an optimal decision point (of splitting the main dataset) - that we have obtained with the PCA decomposition and entropy calculation.


## 4.1.2 - Compute a decision tree for the digit classification and visualize it. You can use “rpart” for creating a tree and “rpart.plot” for visualizing the C5.0 tree.
``` {r 4_1_2}
# if you use the rpart lib
library(rpart)
library(rpart.plot)

datanew <- cbind(id_pca_first_5, id[,1])
datanew$States <-factor(datanew[,6])
tree_raw <- rpart(States ~ PC1 + PC2 + PC3 + PC4 + PC5, data = datanew, method = "class")
summary(tree_raw)
# Plot the tree

# rpart.plot(tree_raw)
# 
library(rattle)
library(rpart.plot)
library(RColorBrewer)
# 
# # plot mytree
fancyRpartPlot(tree_raw, caption = NULL)
# https://community.rstudio.com/t/decision-tree-rpart-summary-interpretation/9996

# # if you use the C50 lib
# library(C50)
# treeModel <- C5.0(x = id[, -1], y = id[,1])
# plot(treeModel)

```
To understand the plot we need to know few things:
top - is the inspected class
0.x is the probability of the Y% samples belonging to the each label (class)
Data is splitted by the thresholds calculated from the previous exercise. Here instead of "information gain" they are using the "Gini index".

Ok from the top:
we are starting with 100% of data -> 8000 of rows (digits) and we are checking what are the probabilities of the rows to be 0 (digit on the top of rectangle - it shows the class with the highest probability for the data to belong to).
this 100% of samples has 0.1 probability to belong to the every class, but the 0 is written on the top, because it supposed to be the "highest" prob.

then we want to split the data into 2 parts. We are cheking which PCx has the highest "improvement" ~ related to Gini index, which does same thing as "Information Gain"
So we basically split the data based on the threshold of the PCx with most importance.
Rows, that are below thershold go to the left side, and others go to the right -> in next boxes you dont have 100% of data, but the percentage of the overall data that fitted into the threshold filtering.

The whole tree shows us probabilities of classification of rows, based on the thresholding the data with the entropy of the PC.


## 4.1.3 – Using the full data set (i.e. dataset from multiple people), evaluate a trained decision tree using cross validation. Try to train a tree with PCA, and without PCA (raw data). Discuss the important parameters.
``` {r cross_validation_full_dataset}

################################################################# Exercise 4.1.3 #################################################################

library(class)
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(plyr)
library(rfUtilities)

# Train the tree without PCA
tree_raw <- rpart(V1 ~ ., data = id, method = "class")
summary(tree_raw)
max_depth_raw <- tree_raw$control$maxdepth
fancyRpartPlot(tree_raw, caption = NULL)

# tree with the PCA
datanew <- cbind(id_pca_first_5, id[,1])
datanew$States <-factor(datanew[,6])
tree_pca <- rpart(States ~ PC1 + PC2 + PC3 + PC4 + PC5, data = datanew, method = "class")
max_depth_pca <- tree_pca$control$maxdepth

# new data for evaluation
id2 <- do.call(rbind, idList[6:10])
id2 <- as.data.frame(id2)
id2[,1] <- factor(id2[,1])

## Predicting the outcome using the full tree_raw
predictions_raw <- predict(tree_raw, id2[,-1], type = "class") # Test, remember to set output as "class"
cf_raw <- confusionMatrix(predictions_raw, id2[,1])
accuracy_predict_raw <- sum(diag(cf_raw$table))/sum(cf_raw$table)
# and pca tree
id2_sn <- as.data.frame(lapply(id2[-1], normalize))
id2_pca <- prcomp(id2_sn, center = TRUE, scale. = TRUE) 
id2_pca_first_5 <- id2_pca$x[, 1:5] # Take first five scores
id2_pca_first_5 <- data.frame(id2_pca_first_5) # Make it a data frame
id2_pca_first_5 <- cbind(id2_pca_first_5, id2[,1])
id2_pca_first_5$States <-factor(id2_pca_first_5[,6])

predictions_pca <- predict(tree_pca, id2_pca_first_5, type = "class") # Test, remember to set output as "class"
cf_pca <- confusionMatrix(predictions_pca, id2[,1])
accuracy_predict_pca <- sum(diag(cf_pca$table))/sum(cf_pca$table)

barplot(c(accuracy_predict_raw, accuracy_predict_pca),
main="comparison of accuracy of the trees using 10-fold cross validation",
xlab="Raw pixel data on the left and normalized PCs data on the right",
ylab="accuracy given new, unseed data",
border="red",
col="blue",
density=10
)

```

```{r}
# https://towardsdatascience.com/how-to-find-decision-tree-depth-via-cross-validation-2bf143f0f3d6
# https://inferate.blogspot.com/2015/05/k-fold-cross-validation-with-decision.html
# https://www.guru99.com/r-decision-trees.html
set.seed(123)
# CROSS VALIDATION AVERAGE ERROR FOR PCA DATA
folds <- split(datanew, cut(sample(1:nrow(datanew)),10))
errs_PCA <- rep(NA, length(folds))

for (tree_depth in treeModel_pca$control$maxdepth:1) 

for (i in 1:length(folds)) {
 test <- ldply(folds[i], data.frame)
 train <- ldply(folds[-i], data.frame)
 treeModel_pca <- rpart(States ~ PC1 + PC2 + PC3 + PC4 + PC5, data = datanew, method = "class",  control =rpart.control(maxdepth = 30))

 tmp.predict <- predict(treeModel_pca, newdata = test, type = "class")
 conf.mat <- table(test$States, tmp.predict)
 errs_PCA[i] <- 1-sum(diag(conf.mat))/sum(conf.mat)
}
plot(1:length(folds), errs_PCA, xlab="folds",ylab=sprintf("average error using k-fold cross-validation: %.3f percent", 100*mean(errs_PCA)))


# CROSS VALIDATION AVERAGE ERROR FOR RAW DATA


```

################################################################# 4.2 Random Forests #################################################################
## 4.2 Create a Random Forest classifier and evaluate it using cross validation. Discuss the critical parameters of “randomForest” (e.g., number and depth of trees)
```{r 4_2_randomforest}
library(party)
library(randomForest)
library(rfUtilities)

# load("idList-co-100.Rdata")
# 
# id <- do.call(rbind, idList[1:5])
# id <- as.data.frame(id)
# id[,1] <- factor(id[,1])
# 
# id2 <- do.call(rbind, idList[6:10])
# id2 <- as.data.frame(id2)
# id2[,1] <- factor(id2[,1])

model.randomforest <- randomForest(V1 ~ ., data = id, ntree=100)
p <- predict(model.randomforest,id2)

cf <- confusionMatrix(p, id2[,1])
print( sum(diag(cf$table))/sum(cf$table) )

plot(model.randomforest) # Error as a function of trees

# 10-fold cross validation
model.cv <- rf.crossValidation(x = model.randomforest, xdata = id, p = 0.1, n = 10) 
# Plot cross validation verses model producers accuracy
par(mfrow=c(1,2)) 
plot(model.cv, type = "cv", main = "accuracy from cv")
plot(model.cv, type = "model", main = "accuracy from model")

# Plot cross validation verses model oob
par(mfrow=c(1,2)) 
plot(model.cv, type = "cv", stat = "oob", main = "oob error cross-validation")
plot(model.cv, type = "model", stat = "oob", main = "oob error model")
```
```{r test}
# plot(model.randomforest)
# legend("top", colnames(model.randomforest$err.rate),col=1:11,cex=0.8,fill=1:11)

library(data.table)
library(ggplot2)

# Get OOB data from plot and coerce to data.table
oobData = as.data.table(plot(model.randomforest))

# Define trees as 1:ntree
oobData[, trees := .I]

# Cast to long format
oobData2 = melt(oobData, id.vars = "trees")
setnames(oobData2, "value", "error")

# Plot using ggplot
ggplot(data = oobData2, aes(x = trees, y = error, color = variable)) + geom_line()

graph2 +
  geom_label_repel(data = filter(mydf, Quarter == second_to_last_quarter), 
                   aes(label = State),
                   nudge_x = .75,
                   na.rm = TRUE) +
  theme(legend.position = "none")


onlyOOB <- oobData2[oobData2$variable == 'OOB', ]
ggplot(data = onlyOOB, aes(x = trees, y = error, color = variable)) + geom_line()
```

``` {r randomforests_p2}
# https://www.statology.org/random-forest-in-r/

#find number of trees that produce lowest test MSE
which.min(model.randomforest$mse)

#find RMSE of best model
sqrt(model.randomforest$mse[which.min(model.randomforest$mse)]) 

#plot the test MSE by number of trees
plot(model.randomforest)

#produce variable importance plot
varImpPlot(model.randomforest) 

# The x-axis displays the average increase in node purity of the regression trees based on splitting on the various predictors displayed on the y-axis.
# From the plot we can see that Wind is the most important predictor variable, followed closely by Temp.
```

``` {r test}
library(randomForest)
library(rfUtilities)
# For classification
data(iris)
iris$Species <- as.factor(iris$Species)    	
set.seed(1234)	
rf.mdl <- randomForest(iris[,1:4], iris[,"Species"], ntree=501) 
rf.cv <- rf.crossValidation(rf.mdl, iris[,1:4], p=0.10, n=99, ntree=30)

# Plot cross validation versus model producers accuracy
par(mfrow=c(1,2)) 
plot(rf.cv, type = "cv", main = "CV producers accuracy")
plot(rf.cv, type = "model", main = "Model producers accuracy")

# Plot cross validation versus model oob
par(mfrow=c(1,2)) 
plot(rf.cv, type = "cv", stat = "oob", main = "CV oob error")
plot(rf.cv, type = "model", stat = "oob", main = "Model oob error")	 

```


