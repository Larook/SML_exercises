---
title: "Exercise_1_KNN"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gmodels)
library(class)
library(caret)
library(swirl)
```

# 4.1 - Decision Trees:
## 4.1.1 - Compute the optimal decision point for the first 5 PCAs of a dataset (e.g. a single person) and compute the information gain associated to it (plot 5 graphs, one for each component, and show the highest information gain). See slides for how to compute information gain.

``` {r PCA}
# define the function for normalization
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# All persons in
load("../data/idList-co-100.Rdata")
id <- do.call(rbind, idList[1:2])  # only one person
id <- as.data.frame(id)
id$V1 <- factor(id$V1)
# id - huge dataframe with everyone as TRAINING

set.seed(423)
shuffled_df <- id[sample(nrow(id)),]
df_pca <- prcomp(x=shuffled_df[,(2:ncol(shuffled_df))], scale = TRUE, center = TRUE, rank.=5)
```
compute the information gain associated to it (plot 5 graphs, one for each component, and show the highest information gain). See slides for how to compute information gain.
```{r info_gain}
# define the function to compute entropy
entropy <- function(S) { # Function to calculate entropy as in the slides
  fullSum <- 0
  for( i in (0:9) ) { 
    if( nrow(S) > 0 ) # Make sure that there is something in the list
      { pi <- nrow(S[ S[,1] == i , ]) / nrow(S) }else{pi <- 0} 
    if(pi > 0 ){
    fullSum <- fullSum - pi * log2(pi)
    }
  }
  return(fullSum)
}

id_pca <- prcomp(x=shuffled_df[,(2:ncol(shuffled_df))], center = TRUE, scale. = TRUE) 

id_pca_first_5 <- id_pca$x[, 1:5] # Take first five scores
id_pca_first_5 <- data.frame(id_pca_first_5) # Make it a data frame

# Calculate starting entropy
entBefore <- entropy(id) 

for( pca_no in (1:5)){
  entList <- c() 
  xList <- c()  
  
  Pts <- seq(min(id_pca_first_5[ ,pca_no]), max(id_pca_first_5[ ,pca_no]), length.out=200)
  for( splitP in (1:length(Pts)) ){
    S1 <- id[ id_pca_first_5[ ,pca_no] < Pts[splitP], ]   # Perform splits from original dataset
    S2 <- id[ id_pca_first_5[ ,pca_no] >= Pts[splitP], ]
    # divide into 2 groups - smaller than current threshold or higher
    s1 <- nrow(S1) 
    s2 <- nrow(S2)
    xList[splitP] <- Pts[splitP]
    ent <- ( s1 * entropy(S1) )/(s1 + s2) + ( s2 * entropy(S2) )/(s1 + s2)  # Calculate entropy
    entList[splitP] <-  entBefore - ent 	# Information gain is calculated 
  }
  plot(xList, entList, xlab="sequence from min to max value of this PC",ylab=paste("Information gain of PC ", pca_no) )
  # TODO: title of plots! + How to understand it?
}
```
## Understanding the information gain plots
x - sequence of min and max values of Principal Components (prcomp.x), we use it to find the best threshold to do classification
y - for every point of x.axis we are calculating the information gain, that we achieve with comparing the entropy before and after PCA decomposition.


we split data based on min and max values of PCA as thresholds-> split rows (means digits)
for every PCA 
we compare then the entropies to choose the optimal point which gives the highest information gain.

So now for every PCA we have an optimal decision point (of splitting the main dataset) - that we have obtained with the PCA decomposition and entropy calculation.

``` {r 4_1_2}
# if you use the rpart lib
library(rpart)
library(rpart.plot)

datanew <- cbind(id_pca_first_5, id[,1])
datanew$States <-factor(datanew[,6])
treeNew <- rpart(States ~ PC1 + PC2 + PC3 + PC4 + PC5, data = datanew, method = "class")
summary(treeNew)
# Plot the tree
rpart.plot(treeNew, box.palette="Blues")

# # if you use the C50 lib
# library(C50)
# treeModel <- C5.0(x = id[, -1], y = id[,1])
# plot(treeModel)

```


```{r information_gain}
# https://www.r-bloggers.com/2020/02/how-is-information-gain-calculated/

# Information Gain (IG) or reduction in entropy from the
# attribute test:

# IG (A) = Entropy before − Entropy after

library(FSelector)
info <- data.frame(fruits = c("watermelon", "apple", "banana", "grape", "grapefruit", "lemon"))
info$sizes <- c("big", "medium", "medium","small" ,"medium", "small")
info$colors <- c("green", "red", "yellow", "green", "yellow", "yellow")
info$shapes <- c("round", "round", "thin", "round", "round", "round")
# get information gain results
information.gain(formula(info), info)



```

## 4.1.2 - Compute a decision tree for the digit classification and visualize it. You can use “rpart” for creating a tree and “rpart.plot” for visualizing the C5.0 tree.

``` {r decision_tree_classification}
datanew <- cbind(id_pca_first_5, id[,1])
# ‘id_pca_first_5’ is a dataframe
datanew$States <-factor(datanew[,6])
tree <- rpart(States ~ PC1 + PC2 + PC3 + PC4 + PC5, data = datanew, method = "class")
rpart.plot(tree)

# or alternatively 

library(C50)
treeModel <- C5.0(x = id[, -1], y = id[,1])
plot(treeModel)
```

## 4.1.3 – Using the full data set (i.e. dataset from multiple people), evaluate a trained decision tree using cross validation. Try to train a tree with PCA, and without PCA (raw data). Discuss the important parameters.
``` {r cross_validation_full_dataset}
predictions <- predict(tree, id_new[,-1], type = "class")
```



